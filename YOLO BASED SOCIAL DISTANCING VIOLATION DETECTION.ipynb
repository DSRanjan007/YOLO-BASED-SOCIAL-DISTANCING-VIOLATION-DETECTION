{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> YOLO BASED SOCIAL DISTANCING VIOLATION DETECTION </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Social-Distancing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Source:** https://www.google.com/search?q=social+distancing+coronavirus&sxsrf=ALeKk02Nhz4SVCHoz4pXx0VtA1tjBNSQTw:1624339411520&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiozPjsv6rxAhU5zDgGHcqEAEsQ_AUoAXoECAEQAw&biw=1920&bih=969#imgrc=a9m2sVtnoygkJM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "### To identify whether the human-beings are maintaining social distance norms or not from a real-time or recorded video footage. It will help to spread awareness among the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. VIDEO**<br>\n",
    "    ***A. Configuration :*** https://www.youtube.com/watch?v=BbZa4OnQrNk&t=3s<br>\n",
    "    ***B. Format        :*** 720p 30FPS (MP4)<br>\n",
    "    ***B. Location      :*** Kempegowda International Airport<br>\n",
    "    \n",
    "    \n",
    "**2. SUPPORTING FILES**<br>\n",
    "    ***A.\"coco.names\"     :*** https://github.com/pjreddie/darknet/blob/master/data/coco.names<br>\n",
    "    ***B.\"yolov3.cfg\"     :*** https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg <br>\n",
    "    ***C.\"yolov3.weights\" :*** confidential<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building:\n",
    "**MODEL USED:** YOLO V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. CPU**<br>\n",
    "    A. Intel i7 9th Gen H(High-Graphics) Processor<br>\n",
    "    B. 6 core 12 Thread<br>\n",
    "    C. 16GB Ram - 2666 MHz<br>\n",
    "    \n",
    "**2. GPU ENGINE**<br>\n",
    "    A. 4GB Nvidia GTX 1650 Discrete Graphics<br>\n",
    "    B. 6 Core <br>\n",
    "    C. CUDA Core 896 (proper setup with OpenCV)<br>\n",
    "    D. Thermal Cooling System 6 Channel<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>----------------------------------------------------------------------------------------------------------</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all important libraries \n",
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize minimum threshold for object detection\n",
    "MIN_CONF   = 0.40\n",
    "NMS_THRESH = 0.50\n",
    "\n",
    "# define the minimum safe distance \n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels present in COCO: \n",
      " ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'] \n",
      "\n",
      "Number of labels :\n",
      " 80 \n",
      "\n",
      "Extracted...\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class \n",
    "labelsPath = os.path.sep.join([\"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "#print the labes\n",
    "print('Labels present in COCO: \\n',LABELS,'\\n')\n",
    "print('Number of labels :\\n',len(LABELS),'\\n')\n",
    "\n",
    "weightsPath = os.path.sep.join([ \"yolov3.weights\"])\n",
    "configPath  = os.path.sep.join([ \"yolov3.cfg\"])\n",
    "\n",
    "# load the YOLO data trained on COCO dataset \n",
    "print(\"Extracted...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine only the \"output\" layer names\n",
    "layer_names  = net.getLayerNames()\n",
    "output_layer = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming the video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS of the current video:  30.0\n",
      "Number of frames in the video:  8408.0\n"
     ]
    }
   ],
   "source": [
    "video_stream = cv2.VideoCapture(\"banglore_airport (2).mp4\")\n",
    "\n",
    "fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS of the current video: \",fps)\n",
    "\n",
    "num_frames = video_stream.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(\"Number of frames in the video: \",num_frames)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Euclidean distance \n",
    "def euclidean_dist(p1, p2):\n",
    "    \n",
    "    return ((p1[0] - p2[0]) ** 2 +  (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "\n",
    "def isclose(p1, p2):\n",
    "    \n",
    "    calculated_distance = euclidean_dist(p1, p2)\n",
    "    calib = (p1[1] + p2[1]) / 2\n",
    "\n",
    "    if 0 < calculated_distance < 0.15 * calib:\n",
    "        return 1\n",
    "\n",
    "    elif 0 < calculated_distance < 0.2 * calib:\n",
    "        return 2\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "height,width=(None,None)\n",
    "q=0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(video_stream.isOpened()):\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, img = video_stream.read()  \n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if width is None or height is None: \n",
    "        height,width = img.shape[:2]\n",
    "        q = width\n",
    "\n",
    "\n",
    "    img = img[0:height, 0:q]\n",
    "    height,width = img.shape[:2]\n",
    "\n",
    "    # Detecting objects \n",
    "\n",
    "    blob  = cv2.dnn.blobFromImage(img,0.00392, (416, 416), (0,0,0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    outs  = net.forward(output_layer)\n",
    "    end   = time.time()\n",
    "\n",
    "     \n",
    "    class_ids   = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outs:\n",
    "\n",
    "        for detection in out:\n",
    "\n",
    "            scores     = detection[5:]\n",
    "            class_id   = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # 0.5 is the threshold for confidence\n",
    "\n",
    "            if confidence > 0.5:\n",
    "\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                \n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.5)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX    \n",
    "\n",
    "    if len(indexes)>0:        \n",
    "\n",
    "        status       = list()        \n",
    "        idf          = indexes.flatten()        \n",
    "        close_pair   = list()        \n",
    "        s_close_pair = list()        \n",
    "        center       = list()        \n",
    "        dist         = list()        \n",
    "\n",
    "        for i in idf:            \n",
    "            (x, y) = (boxes[i][0], boxes[i][1])            \n",
    "            (w, h) = (boxes[i][2], boxes[i][3])            \n",
    "            center.append([int(x + w / 2), int(y + h / 2)])            \n",
    "            status.append(0)            \n",
    "\n",
    "        for i in range(len(center)):            \n",
    "            for j in range(len(center)):                \n",
    "\n",
    "                #compare the closeness of two values\n",
    "                g=isclose(center[i], center[j])                \n",
    "                if g ==1:                    \n",
    "\n",
    "                    close_pair.append([center[i],center[j]])                    \n",
    "                    status[i] = 1                    \n",
    "                    status[j] = 1                    \n",
    "\n",
    "                elif g == 2:                    \n",
    "\n",
    "                    s_close_pair.append([center[i], center[j]])                    \n",
    "\n",
    "                    if status[i] != 1:                        \n",
    "                        status[i] = 2                        \n",
    "\n",
    "                    if status[j] != 1:                        \n",
    "                        status[j] = 2\n",
    "\n",
    "        total_p = len(center)           \n",
    "        high_risk_p = status.count(1)        \n",
    "        safe_p = status.count(0)        \n",
    "        kk = 0        \n",
    "\n",
    "        for i in idf:            \n",
    "\n",
    "            cv2.putText(img, \"Social Distancing Detection\", (0, 50),font,  1, (255, 255, 255), 2)                      \n",
    "            sub_img = img[height - 120:height-20, 0:500]\n",
    "\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])            \n",
    "            (w, h) = (boxes[i][2], boxes[i][3])        \n",
    "\n",
    "            #color of the ractangle when is too close \n",
    "\n",
    "            if status[kk] == 1:                \n",
    "\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 150), 2)\n",
    "\n",
    "            else:                \n",
    "\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    \n",
    "            kk += 1\n",
    "       \n",
    "    cv2.imshow('image',img)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.waitKey(1)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    output = cv2.VideoWriter('output_video_model1.avi',fourcc, 30.0, (img.shape[1], img.shape[0]))\n",
    "    output.write(img)\n",
    "\n",
    "\n",
    "video_stream.release()\n",
    "output.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT IMAGE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"banglore_airport_output1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the function to detect people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, output_layer, personIdx=0):\n",
    "    # dimensions of the frame \n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(output_layer)\n",
    "\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            \n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "           \n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                \n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "               \n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "              \n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the output frames as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = 1\n",
    "output = \"output_video_banglore_airport1.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = video_stream.read()\n",
    "\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # resize the frame and then detect people (and only people) in it\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detect_people(frame, net, output_layer, personIdx=LABELS.index(\"person\"))\n",
    "    height,width=frame.shape[:2]\n",
    "\n",
    "    violate = set()\n",
    "\n",
    "   \n",
    "    if len(results) >= 2:\n",
    "        \n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "       \n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i + 1, D.shape[1]):\n",
    "              \n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                   \n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "       \n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (0, 255, 0)\n",
    "       \n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "        \n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        cv2.putText(frame, str(round(prob * 100))+\"%\", (startX - 5, startY - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 0), 1)\n",
    "        cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "        \n",
    "\n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    text = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "    cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 255, 255), 2)\n",
    "    cv2.putText(frame,'Total People Detected:{}'.format(len(results)), (10, height - 75),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "    \n",
    "\n",
    "    if display > 0:\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "   \n",
    "    if output != \"\" and writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the function to detect vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_vehicle(frame, net, output_layer, personIdx=0):\n",
    "    # dimensions of the frame \n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(output_layer)\n",
    "\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            \n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "           \n",
    "            if classID in personIdx and confidence > MIN_CONF:\n",
    "                \n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "               \n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "              \n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    \n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the output frames as a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = 1\n",
    "output = \"output_video_model2.avi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = video_stream.read()\n",
    "\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    # resize the frame and then detect people (and only people) in it\n",
    "    frame = imutils.resize(frame, width=700)\n",
    "    results = detect_vehicle(frame, net, output_layer, personIdx=[LABELS.index(\"motorbike\"),LABELS.index(\"bicycle\"),LABELS.index(\"car\"),LABELS.index(\"truck\"),LABELS.index(\"bus\")])\n",
    "    height,width=frame.shape[:2]\n",
    "\n",
    "    violate = set()\n",
    "\n",
    "   \n",
    "    if len(results) >= 2:\n",
    "        \n",
    "        centroids = np.array([r[2] for r in results])\n",
    "        D = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
    "\n",
    "       \n",
    "        for i in range(0, D.shape[0]):\n",
    "            for j in range(i + 1, D.shape[1]):\n",
    "              \n",
    "                if D[i, j] < MIN_DISTANCE:\n",
    "                   \n",
    "                    violate.add(i)\n",
    "                    violate.add(j)\n",
    "\n",
    "    # loop over the results\n",
    "    for (i, (prob, bbox, centroid)) in enumerate(results):\n",
    "       \n",
    "        (startX, startY, endX, endY) = bbox\n",
    "        (cX, cY) = centroid\n",
    "        color = (0, 255, 0)\n",
    "       \n",
    "        if i in violate:\n",
    "            color = (0, 0, 255)\n",
    "        \n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "        cv2.putText(frame, str(round(prob * 100))+\"%\", (startX - 5, startY - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 0), 1)\n",
    "        cv2.circle(frame, (cX, cY), 5, color, 1)\n",
    "        \n",
    "\n",
    "    # draw the total number of social distancing violations on the output frame\n",
    "    text = \"Social Distancing Violations: {}\".format(len(violate))\n",
    "    cv2.putText(frame, text, (10, frame.shape[0] - 25), cv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 255, 255), 2)\n",
    "    cv2.putText(frame,'Total People Detected:{}'.format(len(results)), (10, height - 75),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "    \n",
    "\n",
    "    if display > 0:\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "   \n",
    "    if output != \"\" and writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(output, fourcc, 25, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>-------------------------------------------------END------------------------------------------------</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
